{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating song lyrics using RNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68\n",
      "s\n",
      "[0. 0. 0. 0. 1. 0. 0.]\n",
      "\n",
      "\n",
      " After 0 iterations\n",
      "\n",
      " 3un\n",
      "n\n",
      "Is!sjp)fd\"efDp.a\"UO(eJM[U\n",
      "5mSBin0RTTYh[i\"kswB9a4t've21dkRMqC?Bd]2D\"rYHJB)[5j F] F[jiq!-z3w jxM?ar1WeahF](90h[]E-Ba.2\"OslIW7jwUr-cncp\"Sx-dgB)y)DRzO!FIkFegiJTk-\"SOL8PS]N:IZRP2Vy(S GnrF90Xnk35)WwPfNg\"skCO(t:?satiu8tT,N scZ9de1BymCLDAt!LXW\n",
      "r]-7!7vY3p12pJ yzzcwh]?0epoXS2]Lry!LJ0BL\n",
      "E[X8)zt'X3OLz3vx7VQe(vu0JB ?6S4MBY T2E'?4tunXALm?Xhx\n",
      "GTeRq?]h1kX.!P[4e-!rJ 1!?kCr('PNwYU\"zQ6Zu1j?XFFZ[llpBfTD6p \n",
      "eqT)ETd!pEn.!3[4U1upr1lw3wsnN7ET)Fxdbqs(epg V-Ekc.B4 'zN,3zV40Yooq\n",
      "wk[o!-67jXa4ok]QxXgBc-5AD--c5kigixh(x \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 50000 iterations\n",
      "\n",
      " Every muse in the, clize aJomesed of tha have!]  \n",
      "[Mry! go  \n",
      "Suce abrlaybo they oring hasols me armse- \n",
      "There  \n",
      "That's I neen, he's man llowsurf life and mange  \n",
      "Und amathing me and af in if that mes mp  \n",
      "Things I've sanes me and vond  \n",
      "[Cllite of ernight  \n",
      "[H!\n",
      "\n",
      ", The, heart,  \n",
      "[Eveay runn,  \n",
      "So purne- bplavis a shine  \n",
      "That wally Batwh,\" home treed my gonneliese  \n",
      "[Mcky'co uh a riede sanemar froghes erruck that proull:]  \n",
      "  \n",
      "[Lis..\n",
      "\n",
      ", Seine:]  \n",
      "You can ins what I can' wip the true  \n",
      "My me.  \n",
      "[l \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 100000 iterations\n",
      "\n",
      " \n",
      "My  \n",
      "To furs me I can't blong throung been everything too, make hurds walk got out of you're that I had I stowst chVoon mare  \n",
      "This gown roaclo  \n",
      "Lake by what use it my love to me prince my foow I  \n",
      "No neady  \n",
      "And cry want me I roke you'll get to wake tryone my whist crove my sayise my girl  \n",
      "What too I'm try me  \n",
      "I'm no to suy you don't get  \n",
      "  \n",
      "You len up feet your there way  \n",
      "  \n",
      "Out youldnes  \n",
      "That I love my man toom fasing tome, lawn on up wry  \n",
      "Jift me toom  \n",
      "Wath, was to where heart dant  \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 150000 iterations\n",
      "\n",
      "  the on love's sotply samance  \n",
      "Cout yeah let you dedir's it tistelf just there's not oh, you  \n",
      "Yeah you ceazer bewoy  \n",
      "Everybody so muntond on yeah yuck ooh oh yeah  \n",
      "Everytoom for  \n",
      "Lost the light  \n",
      "  \n",
      "It'll you get ie as chargase you comere  \n",
      "  \n",
      "It's peally little centroe,  \n",
      "  \n",
      "He what yes everyt,  \n",
      "  \n",
      "Everything's news'd you love, anone sor deriin  \n",
      "Through what you cop flow  \n",
      " searing  \n",
      "I was oe you  \n",
      "This not think on yeah  \n",
      "  \n",
      "Everything dyste  \n",
      "  \n",
      "Every on yeah, be lonel lapbling our cus \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "\n",
      " After 200000 iterations\n",
      "\n",
      " nd with drevery  \n",
      "I know I mive you wook a rock tile me a bit mfow hand be gues  \n",
      "I ever  \n",
      "  \n",
      "Thie  \n",
      "All olody coyes to ke me  \n",
      "You walked from the sigat to  \n",
      "I still how baby  \n",
      "Time that you telive, I set something  \n",
      "With you come  \n",
      "I'm take a do I tighthing a light  \n",
      "What worterty  \n",
      "I just go eas to time  \n",
      "Oh no world  \n",
      "  \n",
      "Of homesening  \n",
      "That baby  \n",
      "When mase he wants yourse eyan, thera, theygly me tell  \n",
      "Oh baby, always I heardy I know  \n",
      "To onan  \n",
      "Fongever makes  \n",
      "Line  \n",
      "I know than crazy un \n",
      "\n",
      "-------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m target_vector \u001b[38;5;241m=\u001b[39m one_hot_encoder(target_indices)\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# train the network and get the final hidden state\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m hprev_val, loss_val, _ \u001b[38;5;241m=\u001b[39m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mhprev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdated_gradients\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_state\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mhprev_val\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;66;03m# make predictions on every 500th iteration\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m iteration \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    137\u001b[0m \n\u001b[0;32m    138\u001b[0m     \u001b[38;5;66;03m# length of characters we want to predict\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:971\u001b[0m, in \u001b[0;36mBaseSession.run\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    968\u001b[0m run_metadata_ptr \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_NewBuffer() \u001b[38;5;28;01mif\u001b[39;00m run_metadata \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    970\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 971\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions_ptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mrun_metadata_ptr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    973\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m run_metadata:\n\u001b[0;32m    974\u001b[0m     proto_data \u001b[38;5;241m=\u001b[39m tf_session\u001b[38;5;241m.\u001b[39mTF_GetBuffer(run_metadata_ptr)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1214\u001b[0m, in \u001b[0;36mBaseSession._run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1211\u001b[0m \u001b[38;5;66;03m# We only want to really perform the run if fetches or targets are provided,\u001b[39;00m\n\u001b[0;32m   1212\u001b[0m \u001b[38;5;66;03m# or if the call is a partial run that specifies feeds.\u001b[39;00m\n\u001b[0;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m final_fetches \u001b[38;5;129;01mor\u001b[39;00m final_targets \u001b[38;5;129;01mor\u001b[39;00m (handle \u001b[38;5;129;01mand\u001b[39;00m feed_dict_tensor):\n\u001b[1;32m-> 1214\u001b[0m   results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_targets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfinal_fetches\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1215\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfeed_dict_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1217\u001b[0m   results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1394\u001b[0m, in \u001b[0;36mBaseSession._do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1391\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_tf_sessionprun(handle, feed_dict, fetch_list)\n\u001b[0;32m   1393\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1394\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_run_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1395\u001b[0m \u001b[43m                       \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1396\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1397\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_call(_prun_fn, handle, feeds, fetches)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1401\u001b[0m, in \u001b[0;36mBaseSession._do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1399\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_do_call\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m   1400\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1401\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1402\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mOpError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1403\u001b[0m     message \u001b[38;5;241m=\u001b[39m compat\u001b[38;5;241m.\u001b[39mas_text(e\u001b[38;5;241m.\u001b[39mmessage)\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1384\u001b[0m, in \u001b[0;36mBaseSession._do_run.<locals>._run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1381\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_fn\u001b[39m(feed_dict, fetch_list, target_list, options, run_metadata):\n\u001b[0;32m   1382\u001b[0m   \u001b[38;5;66;03m# Ensure any changes to the graph are reflected in the runtime.\u001b[39;00m\n\u001b[0;32m   1383\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_extend_graph()\n\u001b[1;32m-> 1384\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_tf_sessionrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1385\u001b[0m \u001b[43m                                  \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\tensorflow\\python\\client\\session.py:1477\u001b[0m, in \u001b[0;36mBaseSession._call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1475\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_tf_sessionrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, options, feed_dict, fetch_list, target_list,\n\u001b[0;32m   1476\u001b[0m                         run_metadata):\n\u001b[1;32m-> 1477\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf_session\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTF_SessionRun_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeed_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1478\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mfetch_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1479\u001b[0m \u001b[43m                                          \u001b[49m\u001b[43mrun_metadata\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "\n",
    "df = pd.read_csv(\"data/songdata.csv\")\n",
    "df.head()\n",
    "df.shape[0]\n",
    "len(df[\"artist\"].unique())\n",
    "df[\"artist\"].value_counts()[:10]\n",
    "df[\"artist\"].value_counts().values.mean()\n",
    "data = \", \".join(df[\"text\"])\n",
    "chars = sorted(list(set(data)))\n",
    "vocab_size = len(chars)\n",
    "char_to_ix = {ch: i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i: ch for i, ch in enumerate(chars)}\n",
    "print(char_to_ix[\"s\"])\n",
    "print(ix_to_char[68])\n",
    "vocabSize = 7\n",
    "char_index = 4\n",
    "print(np.eye(vocabSize)[char_index])\n",
    "np.eye(vocabSize)[char_index]\n",
    "\n",
    "\n",
    "def one_hot_encoder(index):\n",
    "    return np.eye(vocab_size)[index]\n",
    "\n",
    "\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "seed_value = 42\n",
    "tf.compat.v1.set_random_seed(seed_value)\n",
    "random.seed(seed_value)\n",
    "\n",
    "inputs = tf.compat.v1.placeholder(\n",
    "    shape=[None, vocab_size], dtype=tf.float32, name=\"inputs\"\n",
    ")\n",
    "targets = tf.compat.v1.placeholder(\n",
    "    shape=[None, vocab_size], dtype=tf.float32, name=\"targets\"\n",
    ")\n",
    "init_state = tf.compat.v1.placeholder(\n",
    "    shape=[1, hidden_size], dtype=tf.float32, name=\"state\"\n",
    ")\n",
    "initializer = tf.random_normal_initializer(stddev=0.1)\n",
    "\n",
    "with tf.compat.v1.variable_scope(\"RNN\", reuse=tf.compat.v1.AUTO_REUSE) as scope:\n",
    "    h_t = init_state\n",
    "    y_hat = []\n",
    "    for t, x_t in enumerate(tf.split(inputs, seq_length, axis=0)):\n",
    "        if t > 0:\n",
    "            scope.reuse_variables()\n",
    "        # input to hidden layer weights\n",
    "        U = tf.compat.v1.get_variable(\n",
    "            \"U\", [vocab_size, hidden_size], initializer=initializer\n",
    "        )\n",
    "        # hidden to hidden layer weights\n",
    "        W = tf.compat.v1.get_variable(\n",
    "            \"W\", [hidden_size, hidden_size], initializer=initializer\n",
    "        )\n",
    "        # output to hidden layer weights\n",
    "        V = tf.compat.v1.get_variable(\n",
    "            \"V\", [hidden_size, vocab_size], initializer=initializer\n",
    "        )\n",
    "        # bias for hidden layer\n",
    "        bh = tf.compat.v1.get_variable(\"bh\", [hidden_size], initializer=initializer)\n",
    "        # bias for output layer\n",
    "        by = tf.compat.v1.get_variable(\"by\", [vocab_size], initializer=initializer)\n",
    "        h_t = tf.tanh(tf.matmul(x_t, U) + tf.matmul(h_t, W) + bh)\n",
    "        y_hat_t = tf.matmul(h_t, V) + by\n",
    "        y_hat.append(y_hat_t)\n",
    "\n",
    "output_softmax = tf.nn.softmax(y_hat[-1])\n",
    "outputs = tf.concat(y_hat, axis=0)\n",
    "\n",
    "loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=targets, logits=outputs)\n",
    ")\n",
    "\n",
    "hprev = h_t\n",
    "\n",
    "minimizer = tf.compat.v1.train.AdamOptimizer()\n",
    "with tf.compat.v1.variable_scope(\"RNN\", reuse=tf.compat.v1.AUTO_REUSE):\n",
    "  gradients = minimizer.compute_gradients(loss)\n",
    "  threshold = tf.constant(5.0, name=\"grad_clipping\")\n",
    "\n",
    "  clipped_gradients = []\n",
    "  for grad, var in gradients:\n",
    "    clipped_grad = tf.clip_by_value(grad, -threshold, threshold)\n",
    "    clipped_gradients.append((clipped_grad, var))\n",
    "\n",
    "  updated_gradients = minimizer.apply_gradients(clipped_gradients)\n",
    "\n",
    "sess = tf.compat.v1.Session()\n",
    "init = tf.compat.v1.global_variables_initializer()\n",
    "sess.run(init)\n",
    "\n",
    "pointer = 0\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "\n",
    "    if pointer + seq_length + 1 >= len(data) or iteration == 0:\n",
    "        hprev_val = np.zeros([1, hidden_size])\n",
    "        pointer = 0\n",
    "\n",
    "    # select input sentence\n",
    "    input_sentence = data[pointer : pointer + seq_length]\n",
    "\n",
    "    # select output sentence\n",
    "    output_sentence = data[pointer + 1 : pointer + seq_length + 1]\n",
    "\n",
    "    # get the indices of input and output sentence\n",
    "    input_indices = [char_to_ix[ch] for ch in input_sentence]\n",
    "    target_indices = [char_to_ix[ch] for ch in output_sentence]\n",
    "\n",
    "    # convert the input and output sentence to a one-hot encoded vectors with the help of their indices\n",
    "    input_vector = one_hot_encoder(input_indices)\n",
    "    target_vector = one_hot_encoder(target_indices)\n",
    "\n",
    "    # train the network and get the final hidden state\n",
    "    hprev_val, loss_val, _ = sess.run(\n",
    "        [hprev, loss, updated_gradients],\n",
    "        feed_dict={inputs: input_vector, targets: target_vector, init_state: hprev_val},\n",
    "    )\n",
    "\n",
    "    # make predictions on every 500th iteration\n",
    "    if iteration % 500 == 0:\n",
    "\n",
    "        # length of characters we want to predict\n",
    "        sample_length = 500\n",
    "\n",
    "        # randomly select index\n",
    "        random_index = random.randint(0, len(data) - seq_length)\n",
    "\n",
    "        # sample the input sentence with the randomly selected index\n",
    "        sample_input_sent = data[random_index : random_index + seq_length]\n",
    "\n",
    "        # get the indices of the sampled input sentence\n",
    "        sample_input_indices = [char_to_ix[ch] for ch in sample_input_sent]\n",
    "\n",
    "        # store the final hidden state in sample_prev_state_val\n",
    "        sample_prev_state_val = np.copy(hprev_val)\n",
    "\n",
    "        # for storing the indices of predicted characters\n",
    "        predicted_indices = []\n",
    "\n",
    "        for t in range(sample_length):\n",
    "\n",
    "            # convert the sampled input sentence into one-hot encoded vector using their indices\n",
    "            sample_input_vector = one_hot_encoder(sample_input_indices)\n",
    "\n",
    "            # compute the probability of all the words in the vocabulary to be the next character\n",
    "            probs_dist, sample_prev_state_val = sess.run(\n",
    "                [output_softmax, hprev],\n",
    "                feed_dict={\n",
    "                    inputs: sample_input_vector,\n",
    "                    init_state: sample_prev_state_val,\n",
    "                },\n",
    "            )\n",
    "\n",
    "            # we randomly select the index with the probabilty distribtuion generated by the model\n",
    "            ix = np.random.choice(range(vocab_size), p=probs_dist.ravel())\n",
    "\n",
    "            sample_input_indices = sample_input_indices[1:] + [ix]\n",
    "\n",
    "            # store the predicted index in predicted_indices list\n",
    "            predicted_indices.append(ix)\n",
    "\n",
    "        # convert the predicted indices to their character\n",
    "        predicted_chars = [ix_to_char[ix] for ix in predicted_indices]\n",
    "\n",
    "        # combine the predcited characters\n",
    "        text = \"\".join(predicted_chars)\n",
    "\n",
    "        # predict the predict text on every 50000th iteration\n",
    "        if iteration % 50000 == 0:\n",
    "            print(\"\\n\")\n",
    "            print(\" After %d iterations\" % (iteration))\n",
    "            print(\"\\n %s \\n\" % (text,))\n",
    "            print(\"-\" * 115)\n",
    "\n",
    "    # increment the pointer and iteration\n",
    "    pointer += seq_length\n",
    "    iteration += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
